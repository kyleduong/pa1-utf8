Another encoding of Unicode is UTF-32, which encodes all Unicode code points in 4 bytes. For things like ASCII, the leading 3 bytes are all 0's. What are some tradeoffs between UTF-32 and UTF-8?

UTF-8 has a leading 10 on all the bytes past the first for multi-byte code points. This seems wasteful â€“ if the encoding for 3 bytes were instead 1110XXXX XXXXXXXX XXXXXXXX (where X can be any bit), that would fit 20 bits, which is over a million code points worth of space, removing the need for a 4-byte encoding. What are some tradeoffs or reasons the leading 10 might be useful? Can you think of anything that could go wrong with some programs if the encoding didn't include this restriction on multi-byte code points?

Some of the tradeoffs between UTF-32 and UTF-8 include that UTF-32 will take up less sapces because it has the ability to store more data (in bits) in one sapce as compared to UTF-8 where you need up to 4 spaces to store 32 bits of information. However, in order to do this, you have to UTF-32 sacrafices a lot of wasted space, with characters that only need 1 byte to be stored wasting the other 24 bits of information. They end up being 0's and you waste storage space, which in turn also slows down the program. 

The reason that the leading 10 might be useful is because it is a clear indicator that the byte currently being read is a continuation byte. Without this clear cut specification, there would be much more abiguity in which bytes are the start of a mutibyte sequence or if its part of the single byte's information. Programs would also need more logic to decypher these mutli-byte code points, which would lead to slower processing. Incorrect byte formations may also be more difficult to detect, making debugging and reading more difficult. However, the trade-off to using the leading 10 is that you are sacraficing bit space to do so, which may cause you to need another byte to store what you want. Essentially, you are trading off storage efficiency for clarity. 